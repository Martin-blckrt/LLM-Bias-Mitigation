{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install transformers==4.28.0 datasets==2.11 evaluate\n!pip install -U accelerate --quiet\n!pip install huggingface_hub","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-19T19:28:25.393205Z","iopub.execute_input":"2023-06-19T19:28:25.393669Z","iopub.status.idle":"2023-06-19T19:29:19.162222Z","shell.execute_reply.started":"2023-06-19T19:28:25.393643Z","shell.execute_reply":"2023-06-19T19:29:19.160910Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport datasets\nimport os\nimport random\nimport evaluate\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom datasets import load_dataset, Dataset, concatenate_datasets\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n\nevaluate.logging.disable_progress_bar()\n\ntoxicity = evaluate.load(\"toxicity\", module_type=\"measurement\")\nprompts_to_use = 500","metadata":{"execution":{"iopub.status.busy":"2023-06-19T19:39:10.045080Z","iopub.execute_input":"2023-06-19T19:39:10.045500Z","iopub.status.idle":"2023-06-19T19:39:25.674967Z","shell.execute_reply.started":"2023-06-19T19:39:10.045465Z","shell.execute_reply":"2023-06-19T19:39:25.673814Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/816 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c87fd7d435034d088f7357482e1864e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc1a47a16fa34b97970099ba0f0405af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27658ca487864d46aa4c4c3366bdd5d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5ba43130a7f4be486f64bf5bea49a8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e87d9614f8b40ad9eff0b14a7a9a713"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7b84d23148b4a93a6e8d99c25f97023"}},"metadata":{}}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T19:30:17.361257Z","iopub.execute_input":"2023-06-19T19:30:17.361680Z","iopub.status.idle":"2023-06-19T19:30:17.384707Z","shell.execute_reply.started":"2023-06-19T19:30:17.361643Z","shell.execute_reply":"2023-06-19T19:30:17.383963Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f002244225c34dc8b7d5071d49eb9d0e"}},"metadata":{}}]},{"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"]=\"d9116b6353c330777a40efe088f5f83cb082f32b\"","metadata":{"execution":{"iopub.status.busy":"2023-06-19T19:39:28.119655Z","iopub.execute_input":"2023-06-19T19:39:28.120070Z","iopub.status.idle":"2023-06-19T19:39:28.126762Z","shell.execute_reply.started":"2023-06-19T19:39:28.120034Z","shell.execute_reply":"2023-06-19T19:39:28.125005Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"device = 'cuda'\nmodel_name = \"google/flan-t5-base\"","metadata":{"execution":{"iopub.status.busy":"2023-06-19T19:39:28.271805Z","iopub.execute_input":"2023-06-19T19:39:28.272351Z","iopub.status.idle":"2023-06-19T19:39:28.277714Z","shell.execute_reply.started":"2023-06-19T19:39:28.272317Z","shell.execute_reply":"2023-06-19T19:39:28.276908Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device) ","metadata":{"execution":{"iopub.status.busy":"2023-06-19T19:39:28.437333Z","iopub.execute_input":"2023-06-19T19:39:28.438294Z","iopub.status.idle":"2023-06-19T19:39:32.400251Z","shell.execute_reply.started":"2023-06-19T19:39:28.438248Z","shell.execute_reply":"2023-06-19T19:39:32.399189Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/a-dataset-for-measuring-social-biases-in-mlms/crows_pairs_anonymized.csv\")\ndataset = pd.DataFrame()\n\ndataset['text'] = np.where(df['stereo_antistereo'] == 'stereo', df['sent_more'], df['sent_less'])\ndataset['answer'] = np.where(df['stereo_antistereo'] == 'stereo', df['sent_less'], df['sent_more'])","metadata":{"execution":{"iopub.status.busy":"2023-06-19T19:39:32.402647Z","iopub.execute_input":"2023-06-19T19:39:32.403356Z","iopub.status.idle":"2023-06-19T19:39:32.440589Z","shell.execute_reply.started":"2023-06-19T19:39:32.403319Z","shell.execute_reply":"2023-06-19T19:39:32.439345Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"dataset = Dataset.from_pandas(dataset)\ndataset = dataset.shuffle(seed=42)\ndataset = dataset.train_test_split(test_size=0.075)\ndataset","metadata":{"execution":{"iopub.status.busy":"2023-06-19T19:39:32.441948Z","iopub.execute_input":"2023-06-19T19:39:32.442272Z","iopub.status.idle":"2023-06-19T19:39:32.477168Z","shell.execute_reply.started":"2023-06-19T19:39:32.442241Z","shell.execute_reply":"2023-06-19T19:39:32.476495Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'answer'],\n        num_rows: 1394\n    })\n    test: Dataset({\n        features: ['text', 'answer'],\n        num_rows: 114\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"text\"], truncation=True), \n                                                                                 batched=True, remove_columns=[\"text\", \"answer\"])\nmax_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\nprint(f\"Max source length: {max_source_length}\")\n\ntokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"answer\"], truncation=True), \n                                                                                  batched=True, remove_columns=[\"text\", \"answer\"])\nmax_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\nprint(f\"Max target length: {max_target_length}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-19T19:39:32.479764Z","iopub.execute_input":"2023-06-19T19:39:32.482059Z","iopub.status.idle":"2023-06-19T19:39:32.839445Z","shell.execute_reply.started":"2023-06-19T19:39:32.482027Z","shell.execute_reply":"2023-06-19T19:39:32.836889Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1508 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1508 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_inference(examples, padding=\"max_length\"):\n    template_start = \"Context : Make a sentence using the words in this string.\\n\\nData : \"\n    template_end = \"\"\n    inputs = [template_start + item + template_end for item in examples]\n    \n    return tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True, return_tensors='pt').to(device)\n\n\ndef generate_output(prompt, num_return_sequences=1):\n    \"\"\"\n    generate x number of outputs from a prompt\n\n    Args:\n        prompt (str): XXX\n        max_length (int, optional): max lenght of output\n        num_return_sequences (int, optional): number of expected reponses. Defaults to 1.\n\n    Returns:\n        [str]: list of answers\n    \"\"\"\n\n    output_sequences=model.generate(\n            input_ids=prompt.input_ids,\n            max_length=max_source_length,\n            num_return_sequences=num_return_sequences,\n            no_repeat_ngram_size=2,\n            #repetition_penalty=1.0,\n            do_sample=True,\n            top_k=50,\n            top_p=0.95,\n            temperature=1.0,\n        )\n    \n    # completions = [tokenizer.decode(output_sequence, skip_special_tokens=True) for output_sequence in output_sequences]\n    completions = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n    return completions","metadata":{"execution":{"iopub.status.busy":"2023-06-19T19:39:32.841150Z","iopub.execute_input":"2023-06-19T19:39:32.844696Z","iopub.status.idle":"2023-06-19T19:39:32.859718Z","shell.execute_reply.started":"2023-06-19T19:39:32.844602Z","shell.execute_reply":"2023-06-19T19:39:32.858475Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def preprocess_function(examples, padding=\"max_length\"):\n    template_start = \"Context : Make the following sentence more neutral.\\n\\nData : \"\n    template_end = \"\"\n    inputs = [template_start + item + template_end for item in examples[\"text\"]]\n    \n    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n    labels = tokenizer(text_target=examples[\"answer\"], max_length=max_target_length, padding=padding, truncation=True)\n    \n    if padding == \"max_length\":\n        labels[\"input_ids\"] = [\n            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n        ]\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2023-06-19T19:39:32.861591Z","iopub.execute_input":"2023-06-19T19:39:32.862233Z","iopub.status.idle":"2023-06-19T19:39:32.874462Z","shell.execute_reply.started":"2023-06-19T19:39:32.862183Z","shell.execute_reply":"2023-06-19T19:39:32.873558Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset = dataset.map(preprocess_function, batched=True)\nprint(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-19T19:39:32.877817Z","iopub.execute_input":"2023-06-19T19:39:32.878121Z","iopub.status.idle":"2023-06-19T19:39:33.340645Z","shell.execute_reply.started":"2023-06-19T19:39:32.878098Z","shell.execute_reply":"2023-06-19T19:39:33.339219Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1394 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/114 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"code","source":"del tokenized_inputs, tokenized_targets, dataset, df","metadata":{"execution":{"iopub.status.busy":"2023-06-19T19:39:33.342856Z","iopub.execute_input":"2023-06-19T19:39:33.346758Z","iopub.status.idle":"2023-06-19T19:39:33.353257Z","shell.execute_reply.started":"2023-06-19T19:39:33.346720Z","shell.execute_reply":"2023-06-19T19:39:33.352430Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    \n    if isinstance(predictions, tuple):\n        predictions = predictions[0]\n    \n    txt_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    \n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    txt_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    return toxicity.compute(predictions=txt_preds, references=txt_labels, aggregation=\"ratio\")","metadata":{"execution":{"iopub.status.busy":"2023-06-19T19:39:33.355334Z","iopub.execute_input":"2023-06-19T19:39:33.357113Z","iopub.status.idle":"2023-06-19T19:39:33.369940Z","shell.execute_reply.started":"2023-06-19T19:39:33.357071Z","shell.execute_reply":"2023-06-19T19:39:33.368999Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"label_pad_token_id = -100\n# Data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=label_pad_token_id,\n    pad_to_multiple_of=8\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T19:39:33.373502Z","iopub.execute_input":"2023-06-19T19:39:33.374673Z","iopub.status.idle":"2023-06-19T19:39:33.384671Z","shell.execute_reply.started":"2023-06-19T19:39:33.374639Z","shell.execute_reply":"2023-06-19T19:39:33.383914Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# small batch size to fit in memory\nbatch_size = 32\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir='PoliteT5Base',\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    #gradient_accumulation_steps=2,\n    #gradient_checkpointing=True,\n    predict_with_generate=True,\n    fp16=False,\n    learning_rate=1e-2,\n    num_train_epochs=75,\n    # logging & evaluation strategies\n    logging_dir=\"./logs\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_steps=50,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    push_to_hub=True\n    # optim='adafactor',\n    # push to hub parameters\n    # report_to=\"tensorboard\",\n    # push_to_hub=False,\n    # hub_strategy=\"every_save\",\n    # hub_model_id=repository_id,\n    # hub_token=HfFolder.get_token(),\n)\n\n# Create Trainer instance\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    compute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T19:39:33.386239Z","iopub.execute_input":"2023-06-19T19:39:33.386911Z","iopub.status.idle":"2023-06-19T19:39:35.778880Z","shell.execute_reply.started":"2023-06-19T19:39:33.386879Z","shell.execute_reply":"2023-06-19T19:39:35.778079Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"/kaggle/working/PoliteT5Base is already a clone of https://huggingface.co/Wazzzabeee/PoliteT5Base. Make sure you pull the latest changes with `repo.git_pull()`.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T19:39:35.779999Z","iopub.execute_input":"2023-06-19T19:39:35.780411Z","iopub.status.idle":"2023-06-19T21:46:33.703868Z","shell.execute_reply.started":"2023-06-19T19:39:35.780367Z","shell.execute_reply":"2023-06-19T21:46:33.697768Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1650' max='1650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1650/1650 2:06:56, Epoch 75/75]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Toxicity Ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.325614</td>\n      <td>0.307018</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.843593</td>\n      <td>0.298246</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.633700</td>\n      <td>0.794442</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.633700</td>\n      <td>0.892050</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.547000</td>\n      <td>0.963008</td>\n      <td>0.263158</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.547000</td>\n      <td>0.971096</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.327900</td>\n      <td>0.996581</td>\n      <td>0.307018</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.327900</td>\n      <td>1.005300</td>\n      <td>0.324561</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.327900</td>\n      <td>1.032575</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.228200</td>\n      <td>0.979831</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.228200</td>\n      <td>1.009319</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.183700</td>\n      <td>1.237981</td>\n      <td>0.324561</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.183700</td>\n      <td>1.188903</td>\n      <td>0.385965</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.154600</td>\n      <td>1.198515</td>\n      <td>0.359649</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.154600</td>\n      <td>1.229600</td>\n      <td>0.350877</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.117800</td>\n      <td>1.139430</td>\n      <td>0.368421</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.117800</td>\n      <td>1.171227</td>\n      <td>0.359649</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.117800</td>\n      <td>1.158560</td>\n      <td>0.403509</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.118500</td>\n      <td>1.926256</td>\n      <td>0.078947</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.118500</td>\n      <td>1.348350</td>\n      <td>0.324561</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.233200</td>\n      <td>1.316280</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.233200</td>\n      <td>1.292618</td>\n      <td>0.350877</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.126700</td>\n      <td>1.269076</td>\n      <td>0.342105</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.126700</td>\n      <td>1.329824</td>\n      <td>0.359649</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.087900</td>\n      <td>1.279492</td>\n      <td>0.350877</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.087900</td>\n      <td>1.282614</td>\n      <td>0.324561</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.087900</td>\n      <td>1.288381</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.074700</td>\n      <td>1.414626</td>\n      <td>0.403509</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.074700</td>\n      <td>1.357713</td>\n      <td>0.359649</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.071400</td>\n      <td>1.266253</td>\n      <td>0.350877</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.071400</td>\n      <td>1.250792</td>\n      <td>0.377193</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.056600</td>\n      <td>1.398015</td>\n      <td>0.403509</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.056600</td>\n      <td>1.400616</td>\n      <td>0.385965</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.056600</td>\n      <td>1.409037</td>\n      <td>0.359649</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.057200</td>\n      <td>1.468093</td>\n      <td>0.324561</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.057200</td>\n      <td>1.425360</td>\n      <td>0.394737</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.045600</td>\n      <td>1.493154</td>\n      <td>0.324561</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.045600</td>\n      <td>1.399388</td>\n      <td>0.298246</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.038500</td>\n      <td>1.451134</td>\n      <td>0.342105</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.038500</td>\n      <td>1.300668</td>\n      <td>0.368421</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.022300</td>\n      <td>1.396116</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.022300</td>\n      <td>1.461882</td>\n      <td>0.324561</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.022300</td>\n      <td>1.399583</td>\n      <td>0.324561</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.019900</td>\n      <td>1.501225</td>\n      <td>0.350877</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.019900</td>\n      <td>1.410382</td>\n      <td>0.324561</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.018000</td>\n      <td>1.585532</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.018000</td>\n      <td>1.460317</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.014600</td>\n      <td>1.533451</td>\n      <td>0.342105</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.014600</td>\n      <td>1.488290</td>\n      <td>0.377193</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.013100</td>\n      <td>1.536582</td>\n      <td>0.298246</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.013100</td>\n      <td>1.576185</td>\n      <td>0.350877</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.013100</td>\n      <td>1.543448</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.007300</td>\n      <td>1.473010</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.007300</td>\n      <td>1.513341</td>\n      <td>0.350877</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.004900</td>\n      <td>1.691199</td>\n      <td>0.350877</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.004900</td>\n      <td>1.637588</td>\n      <td>0.350877</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.002800</td>\n      <td>1.825966</td>\n      <td>0.350877</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.002800</td>\n      <td>1.574820</td>\n      <td>0.350877</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.002800</td>\n      <td>1.663133</td>\n      <td>0.350877</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.002900</td>\n      <td>1.745790</td>\n      <td>0.350877</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>0.002900</td>\n      <td>1.634286</td>\n      <td>0.368421</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>0.002000</td>\n      <td>1.643310</td>\n      <td>0.342105</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>0.002000</td>\n      <td>1.748622</td>\n      <td>0.350877</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>0.001400</td>\n      <td>1.808137</td>\n      <td>0.368421</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.001400</td>\n      <td>1.898738</td>\n      <td>0.394737</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.000700</td>\n      <td>1.881062</td>\n      <td>0.359649</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>0.000700</td>\n      <td>1.854069</td>\n      <td>0.359649</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>0.000700</td>\n      <td>1.823341</td>\n      <td>0.350877</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>0.001000</td>\n      <td>1.774704</td>\n      <td>0.350877</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.001000</td>\n      <td>1.810494</td>\n      <td>0.350877</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>0.000800</td>\n      <td>1.825354</td>\n      <td>0.359649</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>0.000800</td>\n      <td>1.844389</td>\n      <td>0.368421</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>0.000800</td>\n      <td>1.838692</td>\n      <td>0.350877</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>0.000800</td>\n      <td>1.850069</td>\n      <td>0.350877</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.000400</td>\n      <td>1.853568</td>\n      <td>0.342105</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1650, training_loss=0.1277589900272362, metrics={'train_runtime': 7617.8396, 'train_samples_per_second': 13.724, 'train_steps_per_second': 0.217, 'total_flos': 7830307318579200.0, 'train_loss': 0.1277589900272362, 'epoch': 75.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T21:46:33.715207Z","iopub.execute_input":"2023-06-19T21:46:33.725980Z","iopub.status.idle":"2023-06-19T21:47:37.607341Z","shell.execute_reply.started":"2023-06-19T21:46:33.725913Z","shell.execute_reply":"2023-06-19T21:47:37.606572Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"Several commits (2) will be pushed upstream.\nThe progress bars may be unreliable.\nTo https://huggingface.co/Wazzzabeee/PoliteT5Base\n   fb866e1..974edea  main -> main\n\nTo https://huggingface.co/Wazzzabeee/PoliteT5Base\n   974edea..7a0a1f7  main -> main\n\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"'https://huggingface.co/Wazzzabeee/PoliteT5Base/commit/974edea027bfa655373e00140def864e896b6a0e'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}