{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install transformers==4.28.0 datasets==2.11 evaluate\n!pip install -U accelerate --quiet\n!pip install huggingface_hub","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-20T12:41:40.511490Z","iopub.execute_input":"2023-06-20T12:41:40.511783Z","iopub.status.idle":"2023-06-20T12:42:30.194029Z","shell.execute_reply.started":"2023-06-20T12:41:40.511757Z","shell.execute_reply":"2023-06-20T12:42:30.192666Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport datasets\nimport os\nimport random\nimport evaluate\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom datasets import load_dataset, Dataset, concatenate_datasets\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n\nevaluate.logging.disable_progress_bar()\n\ntoxicity = evaluate.load(\"toxicity\", module_type=\"measurement\")\nprompts_to_use = 500","metadata":{"execution":{"iopub.status.busy":"2023-06-20T12:42:30.196323Z","iopub.execute_input":"2023-06-20T12:42:30.196775Z","iopub.status.idle":"2023-06-20T12:42:52.255967Z","shell.execute_reply.started":"2023-06-20T12:42:30.196738Z","shell.execute_reply":"2023-06-20T12:42:52.254977Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/816 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2592004450864651bc6bccf6c1fb3a03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a731fb73616c459894f886d0c1b0cd0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fe85bb739a048018d5784fff1de3b43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63ce320b48ef426fbdb90e1287c4fe09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6544ff95841b46f487ffe7bbddfec44f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b20d8d72162045de9b5e80d91290bfcc"}},"metadata":{}}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-06-20T12:47:35.153020Z","iopub.execute_input":"2023-06-20T12:47:35.153755Z","iopub.status.idle":"2023-06-20T12:47:35.176999Z","shell.execute_reply.started":"2023-06-20T12:47:35.153719Z","shell.execute_reply":"2023-06-20T12:47:35.176247Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d386de5f47684200b725d0c2273ac43e"}},"metadata":{}}]},{"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"]=\"d9116b6353c330777a40efe088f5f83cb082f32b\"","metadata":{"execution":{"iopub.status.busy":"2023-06-20T12:47:40.983804Z","iopub.execute_input":"2023-06-20T12:47:40.984553Z","iopub.status.idle":"2023-06-20T12:47:40.989409Z","shell.execute_reply.started":"2023-06-20T12:47:40.984515Z","shell.execute_reply":"2023-06-20T12:47:40.988647Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"device = 'cuda'\nmodel_name = \"google/flan-t5-small\"","metadata":{"execution":{"iopub.status.busy":"2023-06-20T12:47:41.200009Z","iopub.execute_input":"2023-06-20T12:47:41.202467Z","iopub.status.idle":"2023-06-20T12:47:41.206288Z","shell.execute_reply.started":"2023-06-20T12:47:41.202423Z","shell.execute_reply":"2023-06-20T12:47:41.205334Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device) ","metadata":{"execution":{"iopub.status.busy":"2023-06-20T12:47:41.389679Z","iopub.execute_input":"2023-06-20T12:47:41.390045Z","iopub.status.idle":"2023-06-20T12:47:52.059307Z","shell.execute_reply.started":"2023-06-20T12:47:41.390015Z","shell.execute_reply":"2023-06-20T12:47:52.058568Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"667cac1842ba4245ad973d78eb7a59fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3be313e4aa8414bad5708d3d8e731f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4378458f866c4d4ca66f6e4a1da96d74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f92a7fe8d3574454b5c983a95ae89cf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"170d3b3b9af4421a866bfddad66ce483"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99205b8af6c94c43ad79b01cd46fb066"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca55eb3276cd4840afe40e67cbb7650a"}},"metadata":{}}]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/a-dataset-for-measuring-social-biases-in-mlms/crows_pairs_anonymized.csv\")\ndataset = pd.DataFrame()\n\ndataset['text'] = np.where(df['stereo_antistereo'] == 'stereo', df['sent_more'], df['sent_less'])\ndataset['answer'] = np.where(df['stereo_antistereo'] == 'stereo', df['sent_less'], df['sent_more'])","metadata":{"execution":{"iopub.status.busy":"2023-06-20T12:57:37.729357Z","iopub.execute_input":"2023-06-20T12:57:37.729812Z","iopub.status.idle":"2023-06-20T12:57:37.783836Z","shell.execute_reply.started":"2023-06-20T12:57:37.729774Z","shell.execute_reply":"2023-06-20T12:57:37.782941Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"dataset = Dataset.from_pandas(dataset)\ndataset = dataset.shuffle(seed=42)\ndataset = dataset.train_test_split(test_size=0.075)\ndataset","metadata":{"execution":{"iopub.status.busy":"2023-06-20T12:57:37.926229Z","iopub.execute_input":"2023-06-20T12:57:37.928909Z","iopub.status.idle":"2023-06-20T12:57:38.027654Z","shell.execute_reply.started":"2023-06-20T12:57:37.928869Z","shell.execute_reply":"2023-06-20T12:57:38.026937Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'answer'],\n        num_rows: 1394\n    })\n    test: Dataset({\n        features: ['text', 'answer'],\n        num_rows: 114\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"text\"], truncation=True), \n                                                                                 batched=True, remove_columns=[\"text\", \"answer\"])\nmax_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\nprint(f\"Max source length: {max_source_length}\")\n\ntokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"answer\"], truncation=True), \n                                                                                  batched=True, remove_columns=[\"text\", \"answer\"])\nmax_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\nprint(f\"Max target length: {max_target_length}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-20T12:57:38.099513Z","iopub.execute_input":"2023-06-20T12:57:38.101777Z","iopub.status.idle":"2023-06-20T12:57:38.663593Z","shell.execute_reply.started":"2023-06-20T12:57:38.101742Z","shell.execute_reply":"2023-06-20T12:57:38.662850Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1508 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1508 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_inference(examples, padding=\"max_length\"):\n    template_start = \"Context : Make a sentence using the words in this string.\\n\\nData : \"\n    template_end = \"\"\n    inputs = [template_start + item + template_end for item in examples]\n    \n    return tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True, return_tensors='pt').to(device)\n\n\ndef generate_output(prompt, num_return_sequences=1):\n    \"\"\"\n    generate x number of outputs from a prompt\n\n    Args:\n        prompt (str): XXX\n        max_length (int, optional): max lenght of output\n        num_return_sequences (int, optional): number of expected reponses. Defaults to 1.\n\n    Returns:\n        [str]: list of answers\n    \"\"\"\n\n    output_sequences=model.generate(\n            input_ids=prompt.input_ids,\n            max_length=max_source_length,\n            num_return_sequences=num_return_sequences,\n            no_repeat_ngram_size=2,\n            #repetition_penalty=1.0,\n            do_sample=True,\n            top_k=50,\n            top_p=0.95,\n            temperature=1.0,\n        )\n    \n    # completions = [tokenizer.decode(output_sequence, skip_special_tokens=True) for output_sequence in output_sequences]\n    completions = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n    return completions","metadata":{"execution":{"iopub.status.busy":"2023-06-20T12:57:38.668395Z","iopub.execute_input":"2023-06-20T12:57:38.670663Z","iopub.status.idle":"2023-06-20T12:57:38.681080Z","shell.execute_reply.started":"2023-06-20T12:57:38.670626Z","shell.execute_reply":"2023-06-20T12:57:38.680470Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def preprocess_function(examples, padding=\"max_length\"):\n    template_start = \"Context : Make the following sentence more neutral.\\n\\nData : \"\n    template_end = \"\"\n    inputs = [template_start + item + template_end for item in examples[\"text\"]]\n    \n    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n    labels = tokenizer(text_target=examples[\"answer\"], max_length=max_target_length, padding=padding, truncation=True)\n    \n    if padding == \"max_length\":\n        labels[\"input_ids\"] = [\n            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n        ]\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2023-06-20T12:57:38.686058Z","iopub.execute_input":"2023-06-20T12:57:38.689098Z","iopub.status.idle":"2023-06-20T12:57:38.700031Z","shell.execute_reply.started":"2023-06-20T12:57:38.688887Z","shell.execute_reply":"2023-06-20T12:57:38.699389Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset = dataset.map(preprocess_function, batched=True)\nprint(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-20T12:57:38.705774Z","iopub.execute_input":"2023-06-20T12:57:38.708461Z","iopub.status.idle":"2023-06-20T12:57:39.468157Z","shell.execute_reply.started":"2023-06-20T12:57:38.708429Z","shell.execute_reply":"2023-06-20T12:57:39.467369Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1394 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/114 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"code","source":"del tokenized_inputs, tokenized_targets, dataset, df","metadata":{"execution":{"iopub.status.busy":"2023-06-20T12:57:39.470314Z","iopub.execute_input":"2023-06-20T12:57:39.470917Z","iopub.status.idle":"2023-06-20T12:57:39.476917Z","shell.execute_reply.started":"2023-06-20T12:57:39.470885Z","shell.execute_reply":"2023-06-20T12:57:39.476007Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    \n    if isinstance(predictions, tuple):\n        predictions = predictions[0]\n    \n    txt_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    \n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    txt_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    return toxicity.compute(predictions=txt_preds, references=txt_labels, aggregation=\"ratio\")","metadata":{"execution":{"iopub.status.busy":"2023-06-20T12:57:39.479710Z","iopub.execute_input":"2023-06-20T12:57:39.480037Z","iopub.status.idle":"2023-06-20T12:57:39.493610Z","shell.execute_reply.started":"2023-06-20T12:57:39.480008Z","shell.execute_reply":"2023-06-20T12:57:39.492894Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"label_pad_token_id = -100\n# Data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=label_pad_token_id,\n    pad_to_multiple_of=8\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-20T12:57:39.499423Z","iopub.execute_input":"2023-06-20T12:57:39.501915Z","iopub.status.idle":"2023-06-20T12:57:39.507892Z","shell.execute_reply.started":"2023-06-20T12:57:39.501881Z","shell.execute_reply":"2023-06-20T12:57:39.507277Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# small batch size to fit in memory\nbatch_size = 32\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir='PoliteT5Small',\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    #gradient_accumulation_steps=2,\n    #gradient_checkpointing=True,\n    predict_with_generate=True,\n    fp16=False,\n    learning_rate=1e-2,\n    num_train_epochs=75,\n    # logging & evaluation strategies\n    logging_dir=\"./logs\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_steps=50,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    push_to_hub=True\n    # optim='adafactor',\n    # push to hub parameters\n    # report_to=\"tensorboard\",\n    # push_to_hub=False,\n    # hub_strategy=\"every_save\",\n    # hub_model_id=repository_id,\n    # hub_token=HfFolder.get_token(),\n)\n\n# Create Trainer instance\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    compute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-20T12:57:39.511314Z","iopub.execute_input":"2023-06-20T12:57:39.513804Z","iopub.status.idle":"2023-06-20T12:57:44.151660Z","shell.execute_reply.started":"2023-06-20T12:57:39.513770Z","shell.execute_reply":"2023-06-20T12:57:44.150817Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Cloning https://huggingface.co/Wazzzabeee/PoliteT5Small into local empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-06-20T12:57:44.154048Z","iopub.execute_input":"2023-06-20T12:57:44.154496Z","iopub.status.idle":"2023-06-20T13:54:36.784912Z","shell.execute_reply.started":"2023-06-20T12:57:44.154462Z","shell.execute_reply":"2023-06-20T13:54:36.784164Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlwaffle\u001b[0m (\u001b[33mllm-bias\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230620_125746-z1vvpif5</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/llm-bias/huggingface/runs/z1vvpif5' target=\"_blank\">super-salad-60</a></strong> to <a href='https://wandb.ai/llm-bias/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/llm-bias/huggingface' target=\"_blank\">https://wandb.ai/llm-bias/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/llm-bias/huggingface/runs/z1vvpif5' target=\"_blank\">https://wandb.ai/llm-bias/huggingface/runs/z1vvpif5</a>"},"metadata":{}},{"name":"stderr","text":"You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1650' max='1650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1650/1650 56:10, Epoch 75/75]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Toxicity Ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.664179</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.634705</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.934300</td>\n      <td>0.662300</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.934300</td>\n      <td>0.673749</td>\n      <td>0.307018</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.378300</td>\n      <td>0.720052</td>\n      <td>0.298246</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.378300</td>\n      <td>0.760571</td>\n      <td>0.359649</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.253600</td>\n      <td>0.756745</td>\n      <td>0.280702</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.253600</td>\n      <td>0.861775</td>\n      <td>0.307018</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.253600</td>\n      <td>0.844379</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.183900</td>\n      <td>0.825665</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.183900</td>\n      <td>0.864321</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.124600</td>\n      <td>0.833440</td>\n      <td>0.342105</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.124600</td>\n      <td>0.889532</td>\n      <td>0.324561</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.104200</td>\n      <td>0.963063</td>\n      <td>0.298246</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.104200</td>\n      <td>0.900407</td>\n      <td>0.307018</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.092900</td>\n      <td>0.887831</td>\n      <td>0.298246</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.092900</td>\n      <td>0.900900</td>\n      <td>0.298246</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.092900</td>\n      <td>0.976200</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.074500</td>\n      <td>0.929606</td>\n      <td>0.298246</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.074500</td>\n      <td>0.942943</td>\n      <td>0.324561</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.066800</td>\n      <td>0.977940</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.066800</td>\n      <td>0.973131</td>\n      <td>0.298246</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.049400</td>\n      <td>0.964016</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.049400</td>\n      <td>0.998445</td>\n      <td>0.298246</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.042500</td>\n      <td>0.996648</td>\n      <td>0.307018</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.042500</td>\n      <td>0.986108</td>\n      <td>0.324561</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.042500</td>\n      <td>1.033485</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.043200</td>\n      <td>1.035838</td>\n      <td>0.298246</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.043200</td>\n      <td>1.024432</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.032800</td>\n      <td>1.004980</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.032800</td>\n      <td>0.983819</td>\n      <td>0.298246</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.027700</td>\n      <td>1.057558</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.027700</td>\n      <td>1.071929</td>\n      <td>0.307018</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.027700</td>\n      <td>1.085086</td>\n      <td>0.324561</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.019400</td>\n      <td>0.999224</td>\n      <td>0.324561</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.019400</td>\n      <td>1.145446</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.014500</td>\n      <td>1.117852</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.014500</td>\n      <td>1.058555</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.015700</td>\n      <td>1.063775</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.015700</td>\n      <td>1.154444</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.011400</td>\n      <td>1.152909</td>\n      <td>0.289474</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.011400</td>\n      <td>1.201701</td>\n      <td>0.324561</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.011400</td>\n      <td>1.078299</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.009600</td>\n      <td>1.198367</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.009600</td>\n      <td>1.183927</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.009400</td>\n      <td>1.117829</td>\n      <td>0.324561</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.009400</td>\n      <td>1.242439</td>\n      <td>0.307018</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.006500</td>\n      <td>1.173962</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.006500</td>\n      <td>0.986010</td>\n      <td>0.307018</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.008100</td>\n      <td>1.255401</td>\n      <td>0.333333</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.008100</td>\n      <td>1.202439</td>\n      <td>0.289474</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.008100</td>\n      <td>1.243992</td>\n      <td>0.280702</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.003500</td>\n      <td>1.239207</td>\n      <td>0.307018</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.003500</td>\n      <td>1.318867</td>\n      <td>0.307018</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.003300</td>\n      <td>1.263489</td>\n      <td>0.289474</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.003300</td>\n      <td>1.236672</td>\n      <td>0.298246</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.003300</td>\n      <td>1.269108</td>\n      <td>0.307018</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.003300</td>\n      <td>1.276189</td>\n      <td>0.307018</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.003300</td>\n      <td>1.249232</td>\n      <td>0.298246</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.002100</td>\n      <td>1.253001</td>\n      <td>0.307018</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>0.002100</td>\n      <td>1.275355</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>0.002000</td>\n      <td>1.381735</td>\n      <td>0.307018</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>0.002000</td>\n      <td>1.388717</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>0.001600</td>\n      <td>1.317240</td>\n      <td>0.324561</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.001600</td>\n      <td>1.348095</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.002300</td>\n      <td>1.310945</td>\n      <td>0.324561</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>0.002300</td>\n      <td>1.290745</td>\n      <td>0.324561</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>0.002300</td>\n      <td>1.292578</td>\n      <td>0.324561</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>0.001400</td>\n      <td>1.312246</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.001400</td>\n      <td>1.335395</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>0.000800</td>\n      <td>1.343961</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>0.000800</td>\n      <td>1.336704</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>0.001100</td>\n      <td>1.345241</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>0.001100</td>\n      <td>1.351392</td>\n      <td>0.315789</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.001100</td>\n      <td>1.350469</td>\n      <td>0.315789</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1650, training_loss=0.07654087480947827, metrics={'train_runtime': 3412.5754, 'train_samples_per_second': 30.637, 'train_steps_per_second': 0.484, 'total_flos': 2125686966681600.0, 'train_loss': 0.07654087480947827, 'epoch': 75.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2023-06-20T13:54:36.789315Z","iopub.execute_input":"2023-06-20T13:54:36.791692Z","iopub.status.idle":"2023-06-20T13:55:03.227687Z","shell.execute_reply.started":"2023-06-20T13:54:36.791651Z","shell.execute_reply":"2023-06-20T13:55:03.226989Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Several commits (2) will be pushed upstream.\nThe progress bars may be unreliable.\nTo https://huggingface.co/Wazzzabeee/PoliteT5Small\n   10a7cac..542c1b8  main -> main\n\nTo https://huggingface.co/Wazzzabeee/PoliteT5Small\n   542c1b8..9660bfd  main -> main\n\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'https://huggingface.co/Wazzzabeee/PoliteT5Small/commit/542c1b86697fc80243e3aafa54c5cb72676bb3a9'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}